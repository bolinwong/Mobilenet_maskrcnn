{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mobilenet_maskrcnn",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bolinwong/Mobilenet_maskrcnn/blob/main/Mobilenet_maskrcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S-_7iWkaUdx"
      },
      "source": [
        "!git clone https://github.com/bolinwong/Mobilenet_maskrcnn.git\n",
        "%cd Mobilenet_maskrcnn\n",
        "!pip3 install -r requirements.txt\n",
        "!python3 coco.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53YMwtByanPx"
      },
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi.git\n",
        "%cd cocoapi/PythonAPI\n",
        "!make\n",
        "%cd ../../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjjno1qKLzn5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbWzCB88aqb5"
      },
      "source": [
        "!pip uninstall keras-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5-cOo1iaqZc"
      },
      "source": [
        "!pip install h5py==2.10.0\n",
        "!pip install q keras==2.1.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVv1-AV0aqUW"
      },
      "source": [
        "import keras\n",
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbCwcFIIaqWo"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWYG7RKdkjtb"
      },
      "source": [
        "# python 2 compability\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Import Python Packages\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import imgaug\n",
        "import json\n",
        "import pandas as pd\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"../\")\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "\n",
        "# Import Mobile Mask R-CNN Packages\n",
        "from mmrcnn.config import Config\n",
        "from mmrcnn import utils\n",
        "import mmrcnn.model as modellib\n",
        "from mmrcnn import visualize\n",
        "from mmrcnn.model import log\n",
        "\n",
        "\n",
        "%matplotlib inline \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JjWl08gaqPK"
      },
      "source": [
        " #Root directory of the project\n",
        "ROOT_DIR = \"/content/drive/MyDrive/Colab Notebooks/mobilenet_contamination\"\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3QmmjCAaqNL"
      },
      "source": [
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg_ZzNOtaqKy"
      },
      "source": [
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mobile_mask_rcnn_coco.h5\")\n",
        "#COCO_MODEL_PATH = os.path.join(r\"D:\\Dokumen Pengguna\\Desktop\\Label_Me\\mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "#if not os.path.exists(COCO_MODEL_PATH):\n",
        " #   utils.download_trained_weights(COCO_MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opCfpPX7aqIR"
      },
      "source": [
        "class CocoConfig(Config):\n",
        "    \"\"\"Configuration for training on MS COCO.\n",
        "    Derives from the base Config class and overrides values specific\n",
        "    to the COCO dataset.\n",
        "    \"\"\"\n",
        "    ## Give the configuration a recognizable name\n",
        "    NAME = \"contamination\"\n",
        "\n",
        "    ## GPU\n",
        "    IMAGES_PER_GPU = 1\n",
        "    GPU_COUNT = 2\n",
        "\n",
        "    ## Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 1  # COCO has 80 classes (1+80)\n",
        "\n",
        "    ## Backbone Architecture\n",
        "    BACKBONE = \"mobilenetv1\"\n",
        "    \n",
        "    BATCH_SIZE = 2\n",
        "    STEPS_PER_EPOCH = 219\n",
        "    ## Size Options\n",
        "    BACKBONE_STRIDES = [4, 8, 16, 32, 64] #ResNet\n",
        "    #BACKBONE_STRIDES = [2, 4, 8, 16, 32]\n",
        "\n",
        "    #RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256) #ResNet\n",
        "    RPN_ANCHOR_SCALES = (8 , 16, 32, 64, 128)\n",
        "\n",
        "    MINI_MASK_SHAPE = (56, 56) #ResNet\n",
        "    #MINI_MASK_SHAPE = (28, 28)\n",
        "\n",
        " \n",
        "\n",
        "    #TRAIN_ROIS_PER_IMAGE = 200 #ResNet\n",
        "    #TRAIN_ROIS_PER_IMAGE = 128\n",
        "\n",
        "    \n",
        "config = CocoConfig()\n",
        "config.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFQ3OnZZaqFf"
      },
      "source": [
        "class MicrocontrollerDataset(utils.Dataset):\n",
        "    def load_dataset(self, dataset_dir):\n",
        "        self.add_class('dataset', 1, 'contamination')\n",
        "        \n",
        "        \n",
        "        #assert subset in [\"train\", \"val\"]\n",
        "       # dataset_dir = os.path.join(dataset_dir, subset)\n",
        "        \n",
        "        # find all images\n",
        "        for i, filename in enumerate(os.listdir(dataset_dir)):\n",
        "            if '.png' in filename:\n",
        "                self.add_image('dataset', \n",
        "                               image_id=i, \n",
        "                               path=os.path.join(dataset_dir, filename), \n",
        "                               annotation=os.path.join(dataset_dir, filename.replace('.png', '.json')))\n",
        "            \n",
        "    def extract_masks(self, filename):\n",
        "        json_file = os.path.join(filename)\n",
        "        with open(json_file) as f:\n",
        "            img_anns = json.load(f)\n",
        "            \n",
        "        masks = np.zeros([700, 1000, len(img_anns['shapes'])], dtype='uint8')\n",
        "        classes = []\n",
        "        for i, anno in enumerate(img_anns['shapes']):\n",
        "            mask = np.zeros([700, 1000], dtype=np.uint8)\n",
        "            cv2.fillPoly(mask, np.array([anno['points']], dtype=np.int32), 1)\n",
        "            masks[:, :, i] = mask\n",
        "            classes.append(self.class_names.index(anno['label']))\n",
        "        return masks, classes\n",
        " \n",
        "    # load the masks for an image\n",
        "    def load_mask(self, image_id):\n",
        "        # get details of image\n",
        "        info = self.image_info[image_id]\n",
        "        # define box file location\n",
        "        path = info['annotation']\n",
        "        # load XML\n",
        "        masks, classes = self.extract_masks(path)\n",
        "        return masks, np.asarray(classes, dtype='int32')\n",
        "    \n",
        "    def image_reference(self, image_id):\n",
        "        info = self.image_info[image_id]\n",
        "        return info['path']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXoa5k6caqDH"
      },
      "source": [
        "# Create training and validation set\n",
        "# train set\n",
        "dataset_train = MicrocontrollerDataset()\n",
        "dataset_train.load_dataset(r\"/content/drive/MyDrive/Contamination2/Train\")\n",
        "dataset_train.prepare()\n",
        "print('Train: %d' % len(dataset_train.image_ids))\n",
        " \n",
        "# test/val set\n",
        "dataset_val = MicrocontrollerDataset()\n",
        "dataset_val.load_dataset(r\"/content/drive/MyDrive/Contamination2/Test\")\n",
        "dataset_val.prepare()\n",
        "print('Validation: %d' % len(dataset_val.image_ids))\n",
        "\n",
        "dataset_test = MicrocontrollerDataset()\n",
        "dataset_test.load_dataset(r\"/content/drive/MyDrive/Contamination2/Test\")\n",
        "dataset_test.prepare()\n",
        "print('Test: %d' % len(dataset_test.image_ids))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68x_8y6fap8x"
      },
      "source": [
        "# Load and display random samples\n",
        "image_ids = np.random.choice(dataset_train.image_ids, 3)\n",
        "for image_id in image_ids:\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi1F2-xn9lSc"
      },
      "source": [
        "Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoaxFJoP4vBy"
      },
      "source": [
        "\n",
        "\n",
        "# Image augmentation (light but constant)\n",
        "augmentation = iaa.Sequential([\n",
        "    iaa.OneOf([ ## geometric transform\n",
        "        iaa.Affine(\n",
        "            scale={\"x\": (0.98, 1.02), \"y\": (0.98, 1.04)},\n",
        "            translate_percent={\"x\": (-0.02, 0.02), \"y\": (-0.04, 0.04)},\n",
        "            rotate=(-45, 45),\n",
        "            \n",
        "        ),\n",
        "\n",
        "    ]),\n",
        "    \n",
        "    iaa.Flipud(0.2), # vertically flip 20% of all images\n",
        "    iaa.Crop(percent=(0, 0.2)),\n",
        "    iaa.OneOf([ ## brightness or contrast\n",
        "        iaa.Multiply((0.9, 1.1)),\n",
        "        iaa.ContrastNormalization((0.9, 1.1)),\n",
        "    ]),\n",
        "    iaa.OneOf([ ## blur or sharpen\n",
        "\n",
        "        iaa.Sharpen(alpha=(0.0, 0.1)),\n",
        "    ]),\n",
        "])\n",
        "\n",
        "# test on the same image as above\n",
        "imggrid = augmentation.draw_grid(image[:, :, 0], cols=5, rows=2)\n",
        "plt.figure(figsize=(30, 12))\n",
        "_ = plt.imshow(imggrid[:, :, 0],cmap='gray')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wsnPQc7a38n"
      },
      "source": [
        "# Create model in training mode\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                          model_dir=MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCL7Xum3qNev"
      },
      "source": [
        "# Which weights to start with?\n",
        "init_weights = \"imagenet\"  # imagenet, coco, or last\n",
        "\n",
        "if init_weights == \"imagenet\":\n",
        "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
        "elif init_weights == \"coco\":\n",
        "    # Load weights trained on MS COCO, but skip layers that\n",
        "    # are different due to the different number of classes\n",
        "    # See README for instructions to download the COCO weights\n",
        "    model.load_weights(DEFAULT_WEIGHTS, by_name=True,\n",
        "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "elif init_weights == \"last\":\n",
        "    # Load the last model you trained and continue training\n",
        "    model.load_weights(model.find_last()[1], by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thzg6OnivTtE"
      },
      "source": [
        "LEARNING_RATE = 0.006\n",
        "\n",
        "# Train Mask-RCNN Model \n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7hR8jA3vPJc"
      },
      "source": [
        "%%time\n",
        "## train heads with higher lr to speedup the learning\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=LEARNING_RATE*2,\n",
        "            epochs=2,\n",
        "            layers='heads',\n",
        "            augmentation=None)  ## no need to augment yet\n",
        "\n",
        "history = model.keras_model.history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMmqRG6nvr_s"
      },
      "source": [
        "%%time\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            epochs=6,\n",
        "            layers='all',\n",
        "            augmentation=augmentation)\n",
        "\n",
        "new_history = model.keras_model.history.history\n",
        "for k in new_history: history[k] = history[k] + new_history[k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZltvDkLo3kMQ"
      },
      "source": [
        "%%time\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=LEARNING_RATE/5,\n",
        "            epochs=20,\n",
        "            layers='all',\n",
        "            augmentation=augmentation)\n",
        "\n",
        "new_history = model.keras_model.history.history\n",
        "for k in new_history: history[k] = history[k] + new_history[k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37KovKjIvuuh"
      },
      "source": [
        "epochs = range(1,len(next(iter(history.values())))+1)\n",
        "pd.DataFrame(history, index=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9ORqNdvv0VL"
      },
      "source": [
        "\n",
        "plt.figure(figsize=(17,5))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.plot(epochs, history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(epochs, history[\"val_loss\"], label=\"Valid loss\")\n",
        "plt.legend()\n",
        "plt.subplot(132)\n",
        "plt.plot(epochs, history[\"mrcnn_class_loss\"], label=\"Train class ce\")\n",
        "plt.plot(epochs, history[\"val_mrcnn_class_loss\"], label=\"Valid class ce\")\n",
        "plt.legend()\n",
        "plt.subplot(133)\n",
        "plt.plot(epochs, history[\"mrcnn_bbox_loss\"], label=\"Train box loss\")\n",
        "plt.plot(epochs, history[\"val_mrcnn_bbox_loss\"], label=\"Valid box loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKgmeWklv3pl"
      },
      "source": [
        "\n",
        "best_epoch = np.argmin(history[\"val_loss\"])\n",
        "print(\"Best Epoch:\", best_epoch + 1, history[\"val_loss\"][best_epoch])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtuTTBeBa3yJ"
      },
      "source": [
        "model.keras_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWZDKo9yv41s"
      },
      "source": [
        "INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKDLHdh4a3vj"
      },
      "source": [
        "class InferenceConfig(CocoConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "\n",
        "model_path = r\"/content/drive/MyDrive/Colab Notebooks/mobilenet_contamination/logs/1024_contamination20210817T1030/mask_rcnn_1024_contamination_0020.h5\"\n",
        "sys.path.append(model_path)\n",
        "\n",
        "# Load trained weights (fill in path to trained weights here)\n",
        "assert model_path != \"\", \"Provide path to trained weights\"\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s1FNgysa3sp"
      },
      "source": [
        "# Load trained weights\n",
        "final_model_path = model_path\n",
        "\n",
        "print(\"Loading weights from \", final_model_path)\n",
        "model.load_weights(final_model_path, by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE2W_u3wvPqw"
      },
      "source": [
        "class DieSolFrameDatasetTwo(utils.Dataset):\n",
        "    def load_dataset(self, dataset_dir):\n",
        "        self.add_class('dataset', 1, 'contamination')\n",
        "\n",
        "        #assert subset in [\"train\", \"val\"]\n",
        "       # dataset_dir = os.path.join(dataset_dir, subset)\n",
        "        \n",
        "        # find all images\n",
        "        for i, filename in enumerate(os.listdir(dataset_dir)):\n",
        "            if '.png' in filename:\n",
        "                self.add_image('dataset', \n",
        "                               image_id=i, \n",
        "                               path=os.path.join(dataset_dir, filename), \n",
        "                               annotation=os.path.join(dataset_dir, filename.replace('.png', '.json')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpaVouGCa3rD"
      },
      "source": [
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OOjKZ_7D34B"
      },
      "source": [
        "Ground Truth Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25_Zba8Ra3nf"
      },
      "source": [
        "# Test on a random image\n",
        "image_id = random.choice(dataset_val.image_ids)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config, \n",
        "                           image_id, use_mini_mask=False)\n",
        "\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
        "                            dataset_train.class_names, figsize=(8, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8yJh0Xna3k3"
      },
      "source": [
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Uc_KVpb7ho"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os \n",
        "\n",
        "# Compute VOC-Style mAP @ IoU=0.5\n",
        "# Running on 10 images. Increase for better accuracy.\n",
        "image_ids = dataset_test.image_ids\n",
        "APs = []\n",
        "precisions_arr = []\n",
        "recalls_arr = []\n",
        "overlaps_arr = []\n",
        "class_ids_arr = []\n",
        "scores_arr = []\n",
        "\n",
        "#ground-truth and predictions lists\n",
        "gt_tot = np.array([])\n",
        "pred_tot = np.array([])\n",
        "\n",
        "for image_id in image_ids:\n",
        "    # Load image and ground truth data\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_test, inference_config,\n",
        "                               image_id, use_mini_mask=False)\n",
        "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
        "    # Run object detection\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    # Compute AP\n",
        "    AP, precisions, recalls, overlaps =\\\n",
        "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "        \n",
        "    #compute gt_tot and pred_tot\n",
        "    gt, pred = utils.gt_pred_lists(gt_class_id, gt_bbox, r['class_ids'], r['rois'])\n",
        "    gt_tot = np.append(gt_tot, gt)\n",
        "    pred_tot = np.append(pred_tot, pred)\n",
        "\n",
        "    #check if the vectors len are equal\n",
        "    print(\"the actual len of the gt vect is : \", len(gt_tot))\n",
        "    print(\"the actual len of the pred vect is : \", len(pred_tot))\n",
        "    \n",
        "    APs.append(AP)\n",
        "    \n",
        "    #Append Precision\n",
        "    for precision in precisions:\n",
        "      precisions_arr.append(precision)\n",
        "\n",
        "    #Append recalls\n",
        "    for recall in recalls:\n",
        "      recalls_arr.append(recall)\n",
        "\n",
        "    #Append overlaps\n",
        "    for overlap in overlaps:\n",
        "      overlaps_arr.append(overlap)\n",
        "\n",
        "    #Append clas_ids\n",
        "    for class_id in r[\"class_ids\"]:\n",
        "      class_ids_arr.append(class_id)\n",
        "\n",
        "    #Append scores\n",
        "    for score in r[\"scores\"]:\n",
        "      scores_arr.append(score)\n",
        "\n",
        "    print(\"The actual mean average precision for the whole images (matterport methode): \", sum(APs)/len(APs))\n",
        " \n",
        "gt_tot=gt_tot.astype(int)\n",
        "pred_tot=pred_tot.astype(int)\n",
        "#save the vectors of gt and pred\n",
        "save_dir = \"output\"\n",
        "gt_pred_tot_json = {\"gt_tot\" : gt_tot, \"pred_tot\" : pred_tot}\n",
        "df = pd.DataFrame(gt_pred_tot_json)\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "df.to_json(os.path.join(save_dir,\"gt_pred_test.json\"))\n",
        "    \n",
        "#print the confusion matrix and compute true postives, false positives and false negative for each class: \n",
        "#ps : you can controle the figure size and text format by choosing the right values\n",
        "tp, fp, fn = utils.plot_confusion_matrix_from_data(gt_tot, pred_tot, dataset_test.class_names, fz=18, figsize=(20,20), lw=0.5)\n",
        "\n",
        "print(\"mAP: \", np.mean(APs))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlgB64fsqjB0"
      },
      "source": [
        "\n",
        "\n",
        "print(\"tp for each class :\",tp)\n",
        "print(\"fp for each class :\",fp)\n",
        "print(\"fn for each class :\",fn)\n",
        "\n",
        "#eliminate the background class (class A) from tps fns and fns lists since it doesn't concern us anymore : \n",
        "del tp[0]\n",
        "del fp[0]\n",
        "del fn[0]\n",
        "print(\"\\n########################\\n\")\n",
        "print(\"tp for each class :\",tp)\n",
        "print(\"fp for each class :\",fp)\n",
        "print(\"fn for each class :\",fn)\n",
        "\n",
        "print(\"\\n########################\\n\")\n",
        "#Precision \n",
        "Precision = np.array(tp)/(np.array(tp)+ np.array(fp))\n",
        "print(\"Precision: \" ,np.mean(Precision))\n",
        "\n",
        "#Recall \n",
        "Recall = np.array(tp)/(np.array(tp)+ np.array(fn))\n",
        "print(\"Recall: \",np.mean(Recall))\n",
        "\n",
        "#F1 Score\n",
        "f1 = (2*Precision*Recall)/(Precision + Recall)\n",
        "print(\"F1 Score: \", np.mean(f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2MG9jZSvmH2"
      },
      "source": [
        "visualize.plot_precision_recall (AP, precisions, recalls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeGwJLelvu2z"
      },
      "source": [
        "loop_count = 5\n",
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(loop_count):    \n",
        "    r = results[0]\n",
        "    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_test.class_names, r['scores'], ax=get_ax())\n",
        "print(\"Keras inferences with %s second in average\" %((time.time() - start_time) / loop_count))\n",
        "#r = results[0]\n",
        "#visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        " #                           dataset_test.class_names, r['scores'], ax=get_ax())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RDm_knSvtfD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A4MQGVBb7X7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgWjV5EIb7VF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gCdlYNfb7SI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}